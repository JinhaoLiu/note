---
title: 5 朴素贝叶斯
toc: false
date: 2017-10-30
---

### 1 朴素贝叶斯

使用近邻算法时，我们很难对分类结果的置信度进行量化。但如果使用的是基于概率的分类算法——贝叶斯算法——那就可以给出分类结果的可能性了：这名运动员有80%的几率是篮球运动员。

近邻算法又称为**被动学习算法**。这种算法只是将训练集的数据保存起来，在收到测试数据时才会进行计算。如果我们有10万首音乐，那每进行一次分类，都需要遍历这10万条记录才行。

贝叶斯算法则是一种**主动学习算法**。它会根据训练集构建起一个模型，并用这个模型来对新的记录进行分类，因此速度会快很多。

贝叶斯算法的两个优点即：

* 能够给出分类结果的置信度；
* 它是一种主动学习算法。

#### 后验概率

我们用$P(h|D)$来表示$D$条件下事件$h$发生的概率。$P(h)$表示事件$h$发生的概率，称为h的先验概率。$P(h|d)$称为后验概率，表示在观察了数据集$d$之后，$h$事件发生的概率是多少。后验概率又称为条件概率。

#### 贝叶斯法则

贝叶斯法则描述了$P(h)$、$P(h|D)$、$P(D)$、以及$P(D|h)$这四个概率之间的关系：

$$P(h|D) = \frac{P(D|h)P(h)}{P(D)}$$


如果我们有$h_1, h_2,...h_n$等事件。计算不同事件发生的概率，

$$P(h_i|D) = \frac{P(D|h_i)P(h_i)}{P(D)}$$

选取最大的概率，就能用作分类了。这种方法叫**最大后验估计**(Maximum A Posteriori, MAP)，记为$h_{MAP}$：

$$h_{MAP} = \arg \max_{h\in H} P(h|D) =  \arg \max_{h\in H} \frac{P(D|h)P(h)}{P(D)}$$

$H$表示所有的事件，所以$h\in H$表示“对于集合中的每一个事件”。整个公式的含义就是：对于集合中的每一个事件，计算出$P(h|D)$的值，并取最大的结果。

可以发现对于所有的事件，公式中的分母都是$P(D)$，因此即便只计算$P(D|h)P(h)$，也可以判断出最大的结果。那么这个公式就可以简化为：

$$h_{MAP} = \arg \max_{h\in H} P(D|h)P(h)$$

#### Example: 手环推荐

现在我们要为iHealth公司发一套推荐系统。iHealth新出产了两件商品：i100和i500。为了收集数据，让购买的用户填写调查问卷，每个问题都对应一个特征：

![](figures/iHealth.jpg)

也就是说已知一位客户的运动目的、当前运动水平、对健身的热情、是否适应高科技产品，下面用朴素贝叶斯来推荐手环型号。


简单分析一下，这里的假设空间$H$是表格第5列：推荐的手环型号，包括i500和i100; $D$是表格前4列的值。

朴素贝叶斯分类器包含两个部分：训练和分类。

<hh>训练</hh>

训练的输出结果应该是：

* 先验概率，如$P(i100) = 0.4$
* 条件概率，如$P(健康|i100) = 0.167$

我们使用如下代码表示先验概率：

```python
self.prior = {'i500': 0.6, 'i100': 0.4}
```

条件概率的表示有些复杂，用嵌套的字典来实现：

```python
{'i500': {1: {'appearance': 0.3333333333333333, 'health': 0.4444444444444444,
                'both': 0.2222222222222222},
        2: {'active': 0.4444444444444444, 'sedentary': 0.2222222222222222,
                'moderate': 0.3333333333333333},
        3: {'aggressive': 0.6666666666666666, 'moderate': 0.3333333333333333},
        4: {'yes': 0.6666666666666666, 'no': 0.3333333333333333}},
'i100': {1: {'both': 0.5, 'health': 0.16666666666666666,
                'appearance': 0.3333333333333333},
        2: {'active': 0.3333333333333333, 'sedentary': 0.5,
                'moderate': 0.16666666666666666},
        3: {'aggressive': 0.16666666666666666, 'moderate': 0.8333333333333334},
        4: {'yes': 0.3333333333333333, 'no': 0.6666666666666666}}}
```

1、2、3、4表示第几列，所以第一行可以解释为购买i500的顾客中运动目的是外表的概率是0.333。

为了计算概率，要进行计数，可以用字典来统计每个型号的次数。

以下是训练用的Python代码：

```python
class NavieBayes:
	"""
	Classification Using naive bayes
	"""

	def __init__(self, bucketPrefix, testBucketNumber, 
	                       dataFormat, separator='\t'):
		"""
		initialize data
		:param bucketPrefix: 分桶数据集文件前缀
		:param testBucketNumber: 测试桶的编号
		:param dataFormat: 数据格式e.g. attr attr attr attr class
		:param separator: 文件分隔符
		"""
		# 总条数
		total = 0
		# 先验概率计数
		classes = {}
		# 后验概率计数
		counts = {}

		# 从文件中读取数据
		self.format = dataFormat.strip().split(separator)
		# 先验概率
		self.prior = {}
		# 条件概率
		self.conditional = {}

		# 遍历十个桶， 十折交叉验证
		for i in range(1, 11):
			# 跳过测试桶
			if i != testBucketNumber:
				filename = "%s-%02i" % (bucketPrefix, i)
				f = open(filename)
				lines = f.readlines()
				f.close()
				for line in lines:
					fields = line.strip().split('\t')
					ignore = []
					vector = []
					for i in range(len(fields)):
						if self.format[i] == 'num':
							vector.append(float(fields[i]))
						elif self.format[i] == 'attr':
							vector.append(fields[i])
						elif self.format[i] == 'comment':
							ignore.append(fields[i])
						elif self.format[i] == 'class':
							category = fields[i]
					# 处理该条记录
					total += 1
					classes.setdefault(category, 0)
					counts.setdefault(category, {})
					classes[category] += 1
					# 处理各个属性
					col = 0
					for columnValue in vector:
						col += 1
						counts[category].setdefault(col, {})
						counts[category][col].setdefault(columnValue, 0)
						counts[category][col][columnValue] += 1

		# 计数结束，开始计算概率

		# 计算先验概率P(h)
		for (category, count) in classes.items():
			self.prior[category] = count / total

		# 计算条件概率P(h|D)
		for (category, columns) in counts.items():
			self.conditional.setdefault(category, {})
			for (col_id, valueCounts) in columns.items():
				self.conditional[category].setdefault(col_id, {})
				for (attrValue, count) in valueCounts.items():
					self.conditional[category][col_id][attrValue] = 
					           count / classes[category]
		self.tmp = counts

	def classify(self, itemVector):
		"""
		返回itemVector所属类别
		"""
		results = []

		for (category, prior) in self.prior.items():
			prob = prior
			col = 1
			for attrValue in itemVector:
				if attrValue not in conditional[category][col]:
					# 属性不存在，返回0概率
					prob = 0
				else:
					prob *= conditional[category][col][attrValue]
				col += 1
			results.append((prob, category))
		# 返回概率最高的结果
		return max(results)[1]

if __name__ == "__main__":
	c = NavieBayes('iHealth/i', 10, 'attr\tattr\tattr\tattr\tclass')
	print(c.classify(['health' 'moderate', 'moderate', 'yes']))
```


#### Example: 美国国会投票数据

[美国国会投票数据](https://archive.ics.uci.edu/ml/datasets/congressional+voting+records)，其中每条记录代表一个选民，第一列是分类名称（democrat, republican），之后是16条法案，用y和n表示该人是否支持。

文件格式如下：

```
democrat
y n y n n y y y y y n n y n n y
democrat
y y y n n y y y y n n n n n y y
republican
y y n y y y n n n y n y y y n n
```

在调用上一节编写的朴素贝叶斯分类器时使用以下dataFormat参数就可以了：

```
"class\tattr\tattr\tattr\tattr\tattr\tattr\tattr\
tattr\tattr\tattr\tattr\tattr\tattr\tattr\tattr\tattr"
```

<hh>概率值为0</hh>

但是，上面的代码在一些特殊情况下会有一些问题。

使用朴素贝叶斯计算得到的概率其实是真实概率的一种估计，而真实概率是对全量数据做统计得到的。比如说，我们需要对所有人都做血液测试，才能得到健康人返回阴性结果的真实概率。显然，对全量数据做统计是不现实的，所以我们会选取一个样本，如1000人，对他们进行测试并计算概率。大部分情况下，这种估计都是接近于真实概率的。但当真实概率非常小时，这种抽样统计的做法就会有问题了。比如说，民主党对网络非法传播法案的否决率是0.03，即$P(S=no|民主党) = 0.03$。如果我们 分别选取十个民主党和共和党人，看他们对该法案的投票情况，你觉得得到的概率会是什么？答案很可能是0。

在朴素贝叶斯中，概率为0的影响是很大的。如果其中⼀个概率值为0，那么最后的乘积也为0。

为了表示方便，我们采用以下公式：

$$P(x|y)=\frac{n_c}{n}$$

其中,$n$表示训练集中y类别的记录数；$n_c$表示$y$类别中值为$x$的记录数。我们的问题是$n_c$可能为0。解决方法是将公示变为以下形式：

$$P(x|y)=\frac{n_c+mp}{n+m}$$

$m$是一个常数，表示等效样本大小。决定常数$m$的方法有很多，我们这里使用值的类别数目来作为$m$，比如投票有赞成和否决两种类别，所以$m$就为2。$p$则是相应的先验概率，比如说赞成和否决的概率分别是0.5，那$p$就是0.5。






### 2 数值型数据

在贝叶斯方法中，之前我们对事物进行了计数，这种计数则是可以度量的。对于数值型的数据要如何计数呢？通常有两种做法：区分类别和高斯分布。

#### 区分类别

我们可以划定几个范围作为分类，如：

* 年龄
    * < 18 
    * 18 - 22 
    * 23 - 30 
    * 31 - 40 
    * 40
* 年薪
    * $200,000
    * 150,000 - 200,000 
    * 100,000 - 150,000 
    * 60,000 - 100,000 
    * 40,000 - 60,000

划分类别后，就可以应用朴素贝叶斯方法了。

#### 高斯分布

属于类别$y_i$的特征$x_i$的概率为


$$P(x_i|y_i) = \frac{1}{\sqrt{2\pi}\sigma_{ij}} e^{\LARGE -\frac{(x_i-u_{ij})^2}{2\sigma^2_{ij}}}$$

为了举例，我们为上面讲述的手环的例子增加一列收入属性。假设我们要计算$P(100k|i500)$的概率，即购买i500的用户中收入是100,000美元的概率。那么这里$u_{ij}, \sigma_{ij}$分别对应的是购买i500的用户的平均收入和收入的标准差。

样本标准差的计算公式是：

$$\sigma = \sqrt{\frac{\sum_i(x_i-\bar x)^2}{\text{card}(x)-1}}$$

注意分母中的是$\text{card}(x)-1$不是$\text{card}(x)$。

在训练朴素贝叶斯分类器时，可以将所有属性的平均值和样本标准差计算出来，而分类阶段使用平均值和样本标准差计算概率密度分布：

```python
def pdf(mean, ssd, x):
    """概率密度函数，计算P(x|y)"""
    ePart = math.pow(math.e, -(x - mean) ** 2 / (2 * ssd ** 2))
    return (1.0 / (math.sqrt(2 * math.pi) * ssd)) * ePart
```

#### Example: 比马印第安人糖尿病

比马印第安人糖尿病数据集在[前一章](../ch4)已经进行了详细描述。该数据集包含了数值型特征，不能直接用上一节代码。也就是说不能直接用计数的方法来计算某个具体特征的值出现的概率，而是要用高斯分布的概率密度函数计算。

```python
import math
import numpy as np


class NaiveBayes:
	"""
	Classification Using naive bayes
	"""

	def __init__(self, bucketPrefix, testBucketNumber, dataFormat, separator='\t'):
		"""
		initialize data
		:param bucketPrefix: 分桶数据集文件前缀
		:param testBucketNumber: 测试桶的编号
		:param dataFormat: 数据格式，形如attr attr attr attr class
		:param separator: 文件分隔符
		"""
		# 对数值型数据进行求和
		totals = {}
		numericValues = {}

		# 对分类型数据进行计数
		total = 0
		# 先验概率计数
		classes = {}
		# 后验概率计数
		counts = {}

		# 从文件中读取数据
		self.format = dataFormat.strip().split(separator)
		# 先验概率
		self.prior = {}
		# 条件概率
		self.conditional = {}

		# 遍历十个桶， 十折交叉验证
		for i in range(1, 11):
			# 跳过测试桶
			if i != testBucketNumber:
				filename = "%s-%02i" % (bucketPrefix, i)
				f = open(filename)
				lines = f.readlines()
				f.close()
				for line in lines:
					fields = line.strip().split('\t')
					ignore = []
					vector = []
					nums = []
					if len(fields) > len(self.format):
						print(fields, filename, line)
						raise Exception("INPUT ERROR")
					for i in range(len(fields)):
						if self.format[i] == 'num':
							nums.append(float(fields[i]))
						elif self.format[i] == 'attr':
							vector.append(fields[i])
						elif self.format[i] == 'comment':
							ignore.append(fields[i])
						elif self.format[i] == 'class':
							category = fields[i]

					# 处理分类型数据
					# 处理该条记录
					total += 1
					classes.setdefault(category, 0)
					counts.setdefault(category, {})
					classes[category] += 1
					# 处理各个属性
					col = 0
					for columnValue in vector:
						col += 1
						counts[category].setdefault(col, {})
						counts[category][col].setdefault(columnValue, 0)
						counts[category][col][columnValue] += 1

					# 处理数值型数据
					col = 0  # column number
					totals.setdefault(category, {})
					numericValues.setdefault(category, {})
					for columnValue in nums:
						col += 1
						totals[category].setdefault(col, 0)
						totals[category][col] += columnValue
						numericValues[category].setdefault(col, [])
						numericValues[category][col].append(columnValue)

		# 计数结束，开始计算概率

		# 计算先验概率P(h)
		for (category, count) in classes.items():
			self.prior[category] = count / total

		# 计算条件概率P(h|D)
		for (category, columns) in counts.items():
			self.conditional.setdefault(category, {})
			for (col_id, valueCounts) in columns.items():
				self.conditional[category].setdefault(col_id, {})
				for (attrValue, count) in valueCounts.items():
					self.conditional[category][col_id][attrValue] = (
							count / classes[category])

		# 计算平均数和样本标准差
		self.means = {}
		self.standard_deviation = {}
		for category, columns in totals.items():
			self.means.setdefault(category, {})
			self.standard_deviation.setdefault(category, {})
			for col, sums in columns.items():
				self.means[category][col] = 
				    sums/len(numericValues[category][col])
				self.standard_deviation[category][col] = 
				    np.sqrt(np.sum(np.power(
				    (np.array(numericValues[category][col]) -
				    self.means[category][col]), 2))/
				    (len(numericValues[category][col]) - 1.0))

	def pdf(self, mean, ssd, x):
		"""概率密度函数，计算P(x|y)"""
		ePart = math.pow(math.e, -(x - mean) ** 2 / (2 * ssd ** 2))
		return (1.0 / (math.sqrt(2 * math.pi) * ssd)) * ePart

	def classify(self, itemVector, numVector):
		"""
		根据itemVector和numVector进行分类
		"""
		results = []

		for (category, prior) in self.prior.items():
			prob = prior
			col = 1
			for attrValue in itemVector:
				if attrValue not in self.conditional[category][col]:
					# 属性不存在，返回0概率
					prob = 0
				else:
					prob *= self.conditional[category][col][attrValue]
				col += 1
			col = 1
			for numValue in numVector:
				#高斯概率密度分布
				prob *= self.pdf(self.means[category][col], 
				        self.standard_deviation[category][col], numValue)
				col += 1
			results.append((prob, category))
		# 返回概率最高的结果
		return max(results)[1]

	def testBucket(self, bucketPrefix, i):
		"""
		测试
		:param bucketPrefix: 分桶数据集文件前缀
		:param i: 编号
		:return: 测试结果
		"""
		filename = "%s-%02i" % (bucketPrefix, i)
		f = open(filename)
		lines = f.readlines()
		f.close()
		results = {}
		for line in lines:
			fields = line.strip().split('\t')
			ignore = []
			vector = []
			nums = []
			if len(fields) > len(self.format):
				print(fields, filename, line)
				raise Exception("INPUT ERROR")
			for i in range(len(fields)):
				if self.format[i] == 'num':
					nums.append(float(fields[i]))
				elif self.format[i] == 'attr':
					vector.append(fields[i])
				elif self.format[i] == 'comment':
					ignore.append(fields[i])
				elif self.format[i] == 'class':
					category = fields[i]
			test_result = self.classify(vector, nums)
			results.setdefault(category, {})
			results[category].setdefault(test_result, 0)
			results[category][test_result] += 1
		return results





def tenfold(bucketPrefix, dataFormat):
	results = {}
	for i in range(1, 11):
		c = NaiveBayes(bucketPrefix, i, dataFormat)
		t = c.testBucket(bucketPrefix, i)
		for (key, value) in t.items():
			results.setdefault(key, {})
			for (ckey, cvalue) in value.items():
				results[key].setdefault(ckey, 0)
				results[key][ckey] += cvalue

	# now print results
	categories = list(results.keys())
	categories.sort()
	print("\n       Classified as: ")
	header = "        "
	subheader = "      +"
	for category in categories:
		header += "% 2s   " % category
		subheader += "-----+"
	print(header)
	print(subheader)
	total = 0.0
	correct = 0.0
	for category in categories:
		row = " %s    |" % category
		for c2 in categories:
			if c2 in results[category]:
				count = results[category][c2]
			else:
				count = 0
			row += " %3i |" % count
			total += count
			if c2 == category:
				correct += count
		print(row)
	print(subheader)
	print("\n%5.3f percent correct" % ((correct * 100) / total))
	print("total of %i instances" % total)



if __name__ == "__main__":
	tenfold("pimaSmall/pimaSmall",
	        "num\tnum\tnum\tnum\tnum\tnum\tnum\tnum\tclass")
```

输出结果如下：

```
       Classified as: 
         0    1   
      +-----+-----+
 0    |  46 |  13 |
 1    |  15 |  26 |
      +-----+-----+

72.000 percent correct
total of 100 instances
```

与kNN相比，朴素贝叶斯准确率更好，而且计算更快。


#### 为什么要叫“朴素贝叶斯”呢？

我们之所以能将多个概率进行相乘是因为这些概率都是具有独立性的。关于贝叶斯准则，可以参见[概率导论](https://techlarry.github.io/note-big-data/prob/ch1/)。


> In simple terms, a naive Bayes classifier assumes that the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature, given the class variable. For example, a fruit may be considered to be an apple if it is red, round, and about 4" in diameter. Even if these features depend on each other or upon the existence of the other features, a naive Bayes classifier considers all of these properties to independently contribute to the probability that this fruit is an apple. [[ref](https://stackoverflow.com/questions/10614754/what-is-naive-in-a-naive-bayes-classifier)]



但是在现实数据挖掘场景中，这种特征变量之间往往不相互独立。

* 运动员例子中，身高和体重不是互相独立的，因为高的人体重也会较高。 
* 地区邮编、收入、年龄，这些特征也不完全独立，一些地区的房屋都很昂贵，一些地区则只有房车：加州帕罗奥图大多是20岁的年轻人，而亚利桑那州则多是退休人员。
*  在音乐基因工程中，很多特征也是不独立的，如果音乐中有很多变音吉他，那小提琴的 概率就降低了。 血液检验的结果中，T4和TSH这两个指标通常是呈反比的。

所以，在使用贝叶斯方法时，我们需要互相独立的特征，但现实生活中很难找到这样的应用，因此我们只能假设他们是独立的了！我们完全忽略了这个问题，因此才称为“朴素的”（天真的）贝叶斯方法。不过事实证明朴素贝叶斯的效果还是很不错的。