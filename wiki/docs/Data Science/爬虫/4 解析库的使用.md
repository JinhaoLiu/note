---
title: 4 解析库的使用
toc: false
date: 2017-10-30
---


使用正则表达式提取页面信息是比较繁琐的，而且容易出现错误。对于网页节点来说，它可以定义id、class或其他的属性，而且节点之间具有层次关系。利用XPath或CSS选择器可以来定位节点，如果我们利用选择器提取节点，然后再获取它的内容或者属性不就可以非常方便的提取信息了吗？

在Python中，我们怎样来实现这个操作呢？Python有强大的解析库：LXML、BeautifulSoup、PyQuery等等。

### 1 XPath的使用
### 2 BeautifulSoup的使用

[BeautifulSoup](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)是一个强大的解析工具，它借助网页的结构和属性等特性来解析网页。

> Beautiful Soup提供一些简单的、Python式的函数来处理导航、搜索、修改分析树等功能。它是一个工具箱，通过解析文档为用户提供需要抓取的数据，因为简单，所以不需要多少代码就可以写出一个完整的应用程序。
> Beautiful Soup自动将输入文档转换为Unicode编码，输出文档转换为UTF-8编码。你不需要考虑编码方式，除非文档没有制定一个编码方式，这时你仅仅需要说明一下原始编码方式就可以了。

#### 基本用法

```Python
from bs4 import BeautifulSoup
import requests
r = requests.get('http://www.datasciencecourse.org/lectures/')
soup = BeautifulSoup(r.text, 'html.parser')
soup.title
# <title>Lectures</title>
soup.title.name
# u'title'
soup.title.string
# u'Lectures'
soup.title.parent.name
# u'head'
soup.p
# <p><i class="fa fa-calendar" style="font-size: 128px"></i>
# <h1>Lectures</h1></p>
# 获取a标签
soup.a
# <a class="navbar-brand" href="http://practicaldatascience.github.io">
# Practical Data Science</a>
# 提取所有的a标签
soup.find_all('a')
# 从a标签提取url
for link in soup.find_all('a'):
    print(link.get('href'))
# 获取页面的所有文字
print(soup.get_text())
```


| 操作 |  |
| --- | --- |
| 选择元素 | soup.title, soup.head, soup.p, soup.a |
| 获取元素名称 | soup.title.name, soup.head.name, soup.a.name  |
| 获取属性 | soup.p['class'], soup.p['name']  |
| 获取内容 | soup.title.string, soup.p.string|
| 获取属性 | soup.p['class'], soup.p['name']  |
| 获取子孙 | soup.p.children, soup.p.descendants  |
| 获取父节点 | soup.a.parent, soup.a.parents  |
| 获取兄弟节点 | soup.a.next_sibling, soup.a.previous_siblings  |
| 查询所有符合条件的元素 | soup.find_all()  |
| 查询符合条件的第一个元素 | soup.find()  |


#### CSS选择器

Beautiful Soup还提供了[CSS选择器](../../Java/Java Web/CSS.md)。使用CSS选择器时，只需要调用<C>select()</C>方法，传入相应的CSS选择器即可。

```Python
soup.select('title')
# [<title>Lectures</title>]
soup.select('table')
```
用<C>select()</C>方法返回的是一个列表，如果只需要返回一个结果，可以用<C>select_one()</C>方法


在chrome浏览器中，点击右键，选择`copy-copy selector`即可。






### 3 PyQuery的使用