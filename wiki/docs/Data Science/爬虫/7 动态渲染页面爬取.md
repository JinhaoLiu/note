---
title: 7 动态渲染页面爬取
toc: false
date: 2017-10-30
---

我们可以直接使用模拟浏览器运行的方式来实现爬取，这样就可以做到在浏览器中看到是什么样，抓取的源码就是什么样，也就是可见即可爬。

Selenium是一个自动化测试工具，利用它可以驱动浏览器执行特定的动作，如点击、下拉等操作，同时还可以获取浏览器当前呈现的页面的源代码 ，做到可见即可爬。 对于一些JavaScript动态渲染的页面来说，此种抓取方式非常有效。

### Selenium的使用

Selenium在使用前需要安装Selenium库和Chrome Drive.

#### 声明浏览器对象

Selenium支持非常多的浏览器，如Chrome、Firefox等. 


```Python
from selenium import webdriver
browser = webdriver.Chrome()
browser = webdriver.Safari()
```

这样我们就完成了浏览器对象的初始化并赋值为browser对象，接下来我们要做的就是调用browser对象，让其执行各个动作，就可以模拟浏览器操作了。

#### 访问页面

我们可以用`get()`方法来请求一个网页，参数传入链接URL即可，比如在这里我们用`get()`方法访问淘宝，然后打印出源代码，代码如下：

```python
browser.get('https://www.taobao.com')
print(browser.page_source)
browser.close()
```

运行之后我们便发现弹出了Chrome浏览器，自动访问了淘宝，然后控制台输出了淘宝页面的源代码，随后浏览器关闭。


#### 查找节点

Selenium 提供了一系列查找节点的方法。 比如，

* `find_element_by_name()`是根据name值获取.
* `find_element_by_id()`是根据id获取。
* `find_elements_by_xpath`是根据Xpath获取。
* `find_elements_by_class_name`是根据class name获取。
* `find_elements_by_css_selector`是根据css selector获取。

例如，想要从淘宝页面中提取搜索框这个节点，

```python
input_first = browser.find_element_by_id('q')
input_second = browser.find_element_by_css_selector('#q')
input_third = browser.find_element_by_xpath('//*[@id="q"]')
```

另外Selenium还提供了通用的`find_element()`方法，它需要传入两个参数，一个是查找的方式 `By`，另一个就是值，实际上它就是`find_element_by_id/name/xpath()`这种方法的通用函数版本，比如`find_element_by_id(id)`就等价于`find_element(By.ID, id)`，二者得到的结果完全一致。

如果要查找所有满足条件的节点，而不是一个节点，那就需要用`find_elements()`这样的方法，方法名称中element多了一个s，注意区分。

例如查找淘宝左侧导航条的所有条目

```python
lis = browser.find_elements_by_css_selector('.service-bd li')
```

和刚才一样，也可可以直接`find_elements()`方法来选择，所以也可以这样来写：

```python
lis = browser.find_elements(By.CSS_SELECTOR, '.service-bd li')
```


#### 获取节点信息

我们可以使用`get_attribute()`方法来获取节点的属性，那么这个的前提就是先选中这个节点。

每个WebEelement节点都有text属性，我们可以通过直接调用这个属性就可以得到节点内部的文本信息了，就相当于BeautifulSoup的`get_text()`方法、PyQuery的`text()`方法。

另外 WebElement 节点还有一些其他的属性，比如id属性可以获取节点id，location 可以获取该节点在页面中的相对位置，tag_name 可以获取标签名称，size 可以获取节点的大小，也就是宽高，这些属性有时候还是很有用的。


#### 节点交互

Selenium可以驱动浏览器来执行一些操作，
     
输入文字用`send_keys()`方法，清空文字用`clear()`方法，另外还有按钮点击，用`click()`方法。


#### 延时等待

在Selenium中, `get()`方法会在网页框架在结束后结束执行，此时如果获取page_source，可能并不是浏览器完全加载完成的页面，如果某些页面有额外的Ajax请求，我们在网页源代码中也不一定能成功获取到。

所以需要延时等待一定时间，确保节点已经加载出来。有两种等待方式：隐式等待，显式等待。

##### 隐式等待

当使用隐式等待执行测试的时候，如果Selenium没有在DOM中找到节点，将继续等待，超出设定时间后，则抛出找不到节点的异常。

```Python
from selenium import webdriver
browser = webdriver.Chrome()
browser.implicitly_wait(10) 
browser.get(url)
browser.find_element_by_css_selector('dsa')
```

##### 显式等待

更好的方法是指定一个节点和最长等待时间。如果在规定时间内加载出了这个节点，就返回查找的节点；如果到了规定时间依然没有加载出该节点，则抛出超时异常。

```Python
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By

browser = webdriver.Chrome()
browser.get(url)
wait = WebDriverWait(browser, 10)
wait.until(EC.element_to_be_clickable(By.CLASS_NAME, 'reactable-next-page')).
```

* frame可见并切换到该frame上
`EC.frame_to_be_available_and_switch_to_it`
* 元素可以点击，常用于按键
`EC.element_to_be_clickable`
* 元素出现，只要一个符合条件的元素加载出来就通过
`EC.presence_of_element_located`
* 元素出现，须所有符合条件的元素都加载出来，这个基本上就是你爬取的最主要内容了
`EC.presence_of_all_elements_located`
* 判断某段文本是否出现在某元素中，常用于判断输入页数与实际高亮页数是否一致
`EC.text_to_be_present_in_element`


#### 前进和后退

Selenium使用`back()`方法后退，使用`forward()`方法前进。

#### 选项卡管理

连续调用`browser.get()方法，在访问页面的时候，会默认启动同一个选项卡。在Selenium中，可以对选项卡进行操作。

```Python
browser.get(url)
browser.execute_script('window.open()')  #开启新选项卡
browser.switch_to_window(browser.window_handles[1]) # 切换到新选项卡
```


#### Application: LeetCode

```Python
from selenium import webdriver
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.by import By
import json
import codecs
import time

def parse(browser, file):
	for element in browser.find_elements_by_xpath('//*[@id="question-app"]/div/div[2]/div[2]/div[2]/table/tbody[1]/tr'):
		id, title, difficulty = element.text.split("\n")
		url = element.find_element_by_xpath('td[3]/div/a').get_attribute("href")
		data = {'id': id.strip(), 'title': title.strip(), 'difficulty': difficulty.strip(), 'url': url}
		print(data)
		file.write(json.dumps(data) + ",\n")
```

