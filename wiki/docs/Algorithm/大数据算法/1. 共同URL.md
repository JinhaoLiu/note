---
title: 1. 共同URL
toc: false
date: 2017-10-10
top: 1
---

给定a，b两个文件，各存放50亿个url，每个url各占64个字节，内存限制是4G，让你找出a,b文件共同的url。


#### 哈希表

可以估计每个文件安的大小为50G×64=320G，远远大于内存限制的4G。所以不可能将其完全加载到内存中处理。考虑采取分而治之的方法。

* 遍历文件a，对每个url求取hash(url)%1000，然后根据所取得的值将url分别存储到1000个小文件（记为$a_0,a1,...a_{999}$）中。这样每个小文件的大约为300M。
* 遍历文件b，采取和a相同的方式将url分别存储到1000各小文件(记为$b_0,b1,...b_{999}$）。这样处理后，所有可能相同的url都在对应的小文件（$a_0 \text{ vs } b_0, a_1 \text{ vs } b_1,..., a_{999} \text{ vs } b_{999}$）中，不对应的小文件不可能有相同的url。然后我们只要求出1000对小文件中相同的url即可。
* 求每对小文件中相同的url时，可以把其中一个小文件的url存储到哈希表中。然后遍历另一个小文件的每个url，看其是否在刚才构建的哈希表中，如果是，那么就是共同的url，存到文件里面就可以了。

#### Bloom filter

如果允许有一定的错误率，可以使用Bloom filter，4G内存大概可以表示340亿bit。将其中一个文件中的url使用Bloom filter映射为这340亿bit，然后挨个读取另外一个文件的url，检查是否与Bloom filter，如果是，那么该url应该是共同的url（注意会有一定的错误率）。


#### 深入

Finding Near-Duplicate Web Pages: A Large-Scale Evaluation of Algorithms, Monika Henzinger

ref https://github.com/Snailclimb/2019_campus_apply/blob/master/notes/%E6%B5%B7%E9%87%8F%E6%95%B0%E6%8D%AE%E5%A4%84%E7%90%86.md

https://my.oschina.net/vdroid/blog/373439

https://zhuanlan.zhihu.com/p/24383239